{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d6c5926",
   "metadata": {},
   "source": [
    "# BME 590 - Workshop 4 - Modular Cloning\n",
    "**Professor:** Emma Chory, Ph.D.\n",
    "\n",
    "**Authors:** \n",
    "Rick Wierenga, Joe Laforet, Stefan Golas, Ben Perry\n",
    "\n",
    "---\n",
    "\n",
    "### Usage Note\n",
    "**Reminder** - You should be running this notebook **locally** on **VS Code** not navigating it through **GitHub**.\n",
    "\n",
    "**Reminder 2** - Remember to run `git pull` before copying this notebook so you get the most recent class updates. See [Section 6 of the class README](https://github.com/chory-lab/bme590-fall-2025#step-6-updating-to-the-latest-version)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dd2291",
   "metadata": {},
   "source": [
    "## Working from Files\n",
    "\n",
    "In lab automation, it can be useful to load instructions or \"work orders\" form a file. \n",
    "\n",
    "For example, imagine a pipeline that takes a traditional bioinformatic analysis pipeline to determine which genes are most expressed in a given experiment, and subsequently uses **mapped locations** of the relevant wells on a liquid handling dock to proceed with an experiment. \n",
    "\n",
    "Being able to not only connect an outside workflow to the robot via file transfer, but also being able to **map biological elements** to their respective wells and plates on a deck can help integrate workflows even further.\n",
    "\n",
    "Another example is golden mutagenesis, a technique that uses the modularity of golden gate cloning to generate diverse plasmid libraries. Libraries of DNA fragments can be assembled as modules to generate combinatorial diversity from a relatively small number of individual parts that can be assembled in a standardized way.\n",
    "\n",
    "This technique [has been used](https://www.nature.com/articles/s41598-019-47376-1) to evolve a number of enzymes in recent years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35250733",
   "metadata": {},
   "source": [
    "### Pandas, Polars, and Golden Gate Assembly\n",
    "\n",
    "One of the most common types of data file is the **.csv** file, or comma separated values file. There are, of course, nearly infinite ways to store information in file formats; however, let's suppose we have a **mapping** of DNA fragment IDs and the wells which each fragment is in on the liquid handling deck.\n",
    "\n",
    "To illustrate this scenario, let's set up our PLR deck. First, imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ae094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "from pylabrobot.liquid_handling.backends.backend import LiquidHandlerBackend\n",
    "from pylabrobot.liquid_handling import LiquidHandler\n",
    "from pylabrobot.liquid_handling.backends import LiquidHandlerChatterboxBackend\n",
    "from pylabrobot.resources.opentrons import OTDeck\n",
    "from pylabrobot.visualizer.visualizer import Visualizer\n",
    "\n",
    "# resources for deck setup\n",
    "from pylabrobot.resources import (\n",
    "    Deck,\n",
    "    set_tip_tracking,\n",
    "    set_volume_tracking,\n",
    "    set_cross_contamination_tracking,\n",
    "    corning_96_wellplate_360ul_flat,\n",
    "    opentrons_96_tiprack_1000ul,\n",
    "    opentrons_24_tuberack_eppendorf_2ml_safelock_snapcap_acrylic\n",
    ")\n",
    "\n",
    "# import os\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a7dc05",
   "metadata": {},
   "source": [
    "Enable error tracking for cross-contamination, volumes, and tips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c56a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_tip_tracking(enabled = True)\n",
    "set_volume_tracking(enabled = True)\n",
    "set_cross_contamination_tracking(enabled = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9816c777",
   "metadata": {},
   "source": [
    "Now define our standard deck visualization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178a0201",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def visualize_deck(deck: Deck,\n",
    "                         backend: LiquidHandlerBackend):\n",
    "    # try setting up the deck with error-catching\n",
    "    try:\n",
    "        lh = LiquidHandler(backend=backend, deck=deck)\n",
    "        vis = Visualizer(resource = lh)\n",
    "        await lh.setup()\n",
    "        await vis.setup()\n",
    "        return lh\n",
    "    except Exception as e:\n",
    "        print(f\"Error! Got excpetion: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed29c013",
   "metadata": {},
   "source": [
    "Now, let's set up our OpenTrons-2 deck with plates for our DNA fragments and a plate to combine them all together on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f3aab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def make_golden_gate_ot2():\n",
    "\n",
    "    # instantiate deck\n",
    "    deck = OTDeck()\n",
    "\n",
    "    # add plates with fragments\n",
    "    plate_slots = [4, 5, 6]\n",
    "    for i, plate_slot in enumerate(plate_slots):\n",
    "        deck.assign_child_at_slot(corning_96_wellplate_360ul_flat(name = f\"fragments_{i}\"), plate_slot)\n",
    "    \n",
    "    # add tip racks\n",
    "    tip_rack_slots = [1, 2, 3]\n",
    "    for i, tip_slot in enumerate(tip_rack_slots):\n",
    "        deck.assign_child_at_slot(opentrons_96_tiprack_1000ul(name = f\"tip_rack_{i}\"), tip_slot)\n",
    "    \n",
    "    # add working plate\n",
    "    deck.assign_child_at_slot(corning_96_wellplate_360ul_flat(name = f\"cloning_plate\"), 10)\n",
    "\n",
    "    return deck\n",
    "\n",
    "# call function\n",
    "deck = await make_golden_gate_ot2()\n",
    "lh = await visualize_deck(deck, LiquidHandlerChatterboxBackend())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25e45ff",
   "metadata": {},
   "source": [
    "Now, assume we have created a mapping file of DNA ID - Plate Name - Well Location and saved it as a `.csv` file. We need some way to **load the file into memory** to see our ID mapping.\n",
    "\n",
    "Luckily, one of the widest used Python libraries is Pandas, which enables you to load data into a **table in memory, known as a DataFrame**, upon which you can do operations.\n",
    "\n",
    "\n",
    "To start, let's **import pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f14134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a67b03",
   "metadata": {},
   "source": [
    "You should have gotten an **error** along these lines (if you did not, you can go ahead and skip this section)\n",
    "\n",
    "```txt\n",
    "---------------------------------------------------------------------------\n",
    "ModuleNotFoundError                       Traceback (most recent call last)\n",
    "Cell In[5], line 1\n",
    "----> 1 import pandas as pd\n",
    "\n",
    "ModuleNotFoundError: No module named 'pandas'\n",
    "```\n",
    "\n",
    "This happens because **pandas is not an internal Python module** and we do not have it installed in our current environment (remember our setup from the class README - section 3.4?). In general, if you would like to install external packages, you need to do so from the terminal in your environment. Let's go ahead and install it\n",
    "\n",
    "---\n",
    "\n",
    "### **Exercise 0.** Pandas Install (5 pts)\n",
    "\n",
    "\n",
    "In this case, we can install pandas into our **conda environment** from the terminal as follows:\n",
    "\n",
    "1. Run `conda activate lab-automation` to activate the lab-automation environment\n",
    "\n",
    "2. Install pandas via `conda install pandas`.\n",
    "\n",
    "3. Now, if needed, **reload your Jupyter Kernel** by clicking **Restart** in VS Code. \n",
    "\n",
    "4. Now, if needed, **run all the cells up to this point again**. You should no longer get a pandas import error since it is installed!\n",
    "\n",
    "**Note:** There is no submission for this problem becuase you will need Pandas for some of the other exercises. You will not recieve the points here only if your other exercises do not include pandas.\n",
    "\n",
    "Now that pandas is installed, let's **load our CSV** into memory using the `read_csv()` function. We need to point this function to our data, which is present in the newest version of the GitHub repo, under the `workshop_4_data/` folder.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Note:** If you have not used pandas before, you may find [this](https://pandas.pydata.org/docs/user_guide/10min.html) tutorial helpful. **You WILL be using Pandas in the exercises for this workshop, so this is recommended reading!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741a82be",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "print(os.path.dirname(cwd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820d1d74",
   "metadata": {},
   "source": [
    "**Important:** You are telling Python where to look for the data you will load. Therefore, `cwd`, when printed above should point to the `bme590-fall-2025`. If it does not, please copy past the directory path and replace the `cwd` varialbe with the string representation of the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da34f78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd() # -> this should point to bme590-fall-2025/assignments. If it doesn't replace the term here with the copy-pasted path location to the parent folder in which this scirpt is.\n",
    "\n",
    "# define the path pointing to our data file\n",
    "csv_path = os.path.join(os.path.dirname(cwd), \"workshop_4_data\", \"fragments.csv\")\n",
    "\n",
    "# read the dataframe to RAM\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# print the first 5 rows of the data\n",
    "df.head() # <- NOTE: df.head(n) prints the first n rows of a dataframe. df.tail(n) does the same but for the last n rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb043b4",
   "metadata": {},
   "source": [
    "As you can see, we have the following columns:\n",
    "\n",
    "- `well_id` - The identifier of the well for each DNA fragment\n",
    "\n",
    "- `fragment_id` - The identifier of the DNA fragment itself\n",
    "\n",
    "- `plate_id` - The identifier of the plate itself.\n",
    "\n",
    "- `volume (uL)` - The volume of that fragment in a given plate well.\n",
    "\n",
    "We can now write a for loop to iterate through each row using `df.iterrows()`. **Note** that this function returns a tuple at each iteration of the form `index, row`, where `row` is a **dictionary** of column_name-value pairs.\n",
    "\n",
    "The below code will run a for loop and show you how to iterrate through the loaded dataframe and extract the values for each column.\n",
    "\n",
    "**Note -** The `break` keyword exits the loop immediately, so only one iteration is done, as an example print statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdbf2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df.iterrows(): # iterate over rows\n",
    "    print(f\"{row.keys()=}\") # print the keys in the row dictionary\n",
    "    print()\n",
    "    print(f\"{row['well_id']=}\") # extract the well id\n",
    "    print(f\"{row['fragment_id']=}\") # extract the fragment id\n",
    "    print(f\"{row['plate_id']=}\") # extract the plate id\n",
    "    print(f\"{row['volume (uL)']=}\") # extract the volume\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ad8363",
   "metadata": {},
   "source": [
    "You can also filter the dataframe by values in certain columns. For example, filtering it so that only plate 2 is present in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2294dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plate_2 = df[df['plate_id'] == 2]\n",
    "df_plate_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f31de9",
   "metadata": {},
   "source": [
    "You can also filter by more specific things, such as getting the plate and well containing a fragment id of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6287ddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_specific_fragment = df[df[\"fragment_id\"] == \"X095\"]\n",
    "df_specific_fragment.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0209d30",
   "metadata": {},
   "source": [
    "Since there are multiple wells containing our fragment of interest, we can **randomly choose one** like this.\n",
    "\n",
    "**Hint -** This is probably going to be useful in the exercises. If there were a scenario where you needd more volume than was present in a singular well, you may need to use information from multiple rows. However, for this workshop, we will keep it simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4a4dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df_specific_fragment.sample(n = 1)\n",
    "row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02aaf02",
   "metadata": {},
   "source": [
    "These are the basics for what you need to complete this workshop, but we encourage you to explore other pandas functionalities.\n",
    "\n",
    "As a side note, if you ever work with **large scale datasets** such as those in the gigabyte range, a useful library is [Polars](https://pola.rs/), which enables large-scale multi-threaded dataframe operations, in a lower level language, and even [on the GPU](https://pola.rs/posts/gpu-engine-release/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f53ff0",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "---\n",
    "\n",
    "**TO-DO:** For each of the following exercses, since they are a bit different from each other, we will require submission of one or more of the following:\n",
    "\n",
    "- a `.txt` file containing your code.\n",
    "\n",
    "- a `.gif` file containing a GIF animation of your protocol running\n",
    "\n",
    "- a `.png` or `.jpg` image of your deck setup, if needed.\n",
    "\n",
    "- a `.csv` file, for some exercises\n",
    "\n",
    "We will explicitly tell you for each exercise and sub-exercise, which items to submit. There will be a **sample submission format** for each exercise.\n",
    "\n",
    "---\n",
    "\n",
    "Some exercises below will ask you to define your own **functions or classes**. We will provide the **function or class name** and sometimes the **input argument names** for you, but in gneeral, the body of the functions is up to you.\n",
    "\n",
    "You should include `time.sleep(x)` calls between every step so you have time to visualize the protocol as it runs. At a minimum, `x = 0.1` for 0.1 s delay. Experiment with this value for one that works for you.\n",
    "\n",
    "Once you get your protocol working as intended for each problem, you will need to **record a GIF** of your protocol running for each exercise, as directed\n",
    "\n",
    "**IMPORTANT** - You should be judiciously commenting your code to explain its function. We will grade every problem by **quality of code**. Excessively long code or lack of comments will be subject to **point deduction**\n",
    "\n",
    "Furthermore, you can write the code how you see fit. However, **do not change function names** and make sure to **include your imports** at the top of your .txt file submission.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f93820c",
   "metadata": {},
   "source": [
    "\n",
    "### **Exercise 1.** Deck Setup (15 pts)\n",
    "\n",
    "First, given the deck we setup, complete the following function `add_fragments()` which should take in the following parameters:\n",
    "\n",
    "- `deck` - The already set up OT-2 deck from earlier in this workshop.\n",
    "\n",
    "- `fragment_csv_path` - The path to the fragment data we were using earlier.\n",
    "\n",
    "Using the techniques learned from the **Liquid handling** workshop about **setting initial liquids**, write a function that puts the correct volume of each fragment in its target well, solely determined by the data in the `fragments.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa09fb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fragments(deck : Deck,\n",
    "                  fragment_csv_path : str):\n",
    "    ... # YOUR CODE HERE\n",
    "\n",
    "\n",
    "# call the function\n",
    "add_fragments(deck, csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cc6e0f",
   "metadata": {},
   "source": [
    "Submit your code as `exercise_1.txt`. Make sure it is commented. Imports are not necessary to include here.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fefbf0",
   "metadata": {},
   "source": [
    "\n",
    "### **Exercise 2.** Work Order Parsing (25 pts)\n",
    "\n",
    "We have now been provided a list of work orders of golden mutagensis targets in the `cloning.csv` file. Let's go ahead and take a look of some example rows of this dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8a4b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path\n",
    "work_order_csv_path = os.path.join(os.path.dirname(cwd), \"workshop_4_data\", \"cloning.csv\")\n",
    "\n",
    "# read the dataframe to RAM\n",
    "work_order_df = pd.read_csv(work_order_csv_path)\n",
    "\n",
    "# print the first 5 rows of the data\n",
    "work_order_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e38c2c1",
   "metadata": {},
   "source": [
    "Great! This file contains two columns:\n",
    "\n",
    "- `well_id` - The well ID on the `cloning_plate` of fragments to combine. **Note -** the well ID index now is of the form A01, which should translate to A1. You will have to find a way to translate these IDs. See hint below.\n",
    "\n",
    "- `fragment_tuple` - A string representation of a tuple of form: `(ID_1,ID_2,ID_3)` where each ID corresponds to a DNA fragment.\n",
    "\n",
    "For this exercise, you will need to write a **generator** called `extract_fragment_combinations()` which creates a for loop over the rows in the dataframe, and yields data in the form: `well_id, fragments` where:\n",
    "\n",
    "- `well_id` is the **converted** id from the form \"A01\" to \"A1\" for all combinations of IDs.\n",
    "\n",
    "- `fragments` is a **tuple** of the form **fragment_X, fragment_Y, fragment_Z**\n",
    "\n",
    "**HOWEVER** Our list was assembled by a mad scientist! Therefore, some of the combinations point to IDs which **don't exist** in the fragments set at all. If that is the case, we should have this function return **None, None**\n",
    "\n",
    "**2.A.** Let's break down this problem piece by piece. First, let's write a function to convert the **well ID** to the right format. This should simply input the `old_well_id` and convert it to a normalized ID. To do this:\n",
    "\n",
    "- Index the first character of the `ID` as the **row_id** for a given number.\n",
    "\n",
    "- For the rest of the string, convert it to an **interger** using the `int()` conversion function.\n",
    "\n",
    "- Finally, using [string concatenation](https://www.w3schools.com/python/gloss_python_string_concatenation.asp) or f-strings, combine the row_id and integer number together and return it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53603093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_well_id(old_well_id: str):\n",
    "    ... # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2989437",
   "metadata": {},
   "source": [
    "**2.B.** Great! Now we need a function called `get_well()` which can input:\n",
    "\n",
    "- `fragment_id` - The identifier of the fragment we are checking is present.\n",
    "\n",
    "- `fragment_df` - The dataframe of fragment IDs and locations, originally loaded as **df**\n",
    "\n",
    "The function should use pandas filtering to:\n",
    "\n",
    "- Filter down the dataframe to only rows where `fragment_id` contains the fragment ID being searched for.\n",
    "\n",
    "- Impelment logic that checks if the length of the filtered dataframe is 0. If so, return `None, None`\n",
    "\n",
    "- Otherwise, sample **only one row** and use the `.item()` function to get the specific cell entry for given row and return a tuple containing data of the form `well_id, plate_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7318c610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_well(fragment_id: str,\n",
    "             fragment_df: pd.DataFrame):\n",
    "    ... # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c8bced",
   "metadata": {},
   "source": [
    "**2.C.** Finally, implement the `extract_fragment_conmbinations` function, which will input the work-order and existing fragments CSV path and:\n",
    "\n",
    "- Load each dataframe into memory using `pd.read_csv(...)`\n",
    "\n",
    "- Iterate through the rows of the work order dataframe using `iterrows()`\n",
    "\n",
    "- For each row, extract the `fragment_tuple` and implement logic to check that each fragment is valid:\n",
    "\n",
    "    - For each fragment in the fragment tuple, call `get_well()`. If `None, None` is returned for any of them, then set a boolean flag named `all_valid` to False.\n",
    "\n",
    "- If `all_valid` is true, **yield** data of the form `well_id, fragment_tuple`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a95b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fragment_combinations(cloning_csv_path: str | os.PathLike,\n",
    "                                  fragment_csv_path: str | os.PathLike):\n",
    "\n",
    "    ... # YOUR CODE HERE\n",
    "\n",
    "    # iterate over work orders\n",
    "    for _, row in ...:\n",
    "\n",
    "        # boolean logic implementation\n",
    "        all_fragments_valid = True \n",
    "        \n",
    "        ... # YOUR CODE HERE\n",
    "\n",
    "        # scaffold logic\n",
    "        for ... in ...:\n",
    "            if ...:\n",
    "                all_fragments_valid = False\n",
    "                break  \n",
    "\n",
    "        # only if fragments are valid, then yield the relevant results.\n",
    "        if all_fragments_valid:\n",
    "            well_id = convert_well_id(...)\n",
    "            yield well_id, fragment_tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7a944d",
   "metadata": {},
   "source": [
    "Great! Submit your code as a file named `exercise_2.txt`. Make sure it is commented. Imports are not necessary here.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31906b02",
   "metadata": {},
   "source": [
    "### **Exercise 3.** Modular Cloning (40 pts)\n",
    "\n",
    "Now that we have the deck setup and a way to iterate over our work-order data, we should now design a liquid handling protocol that will achieve the following:\n",
    "\n",
    "- Iterate over the work order list to get the well for cloning and the triplet of clones to pipette together.\n",
    "\n",
    "- For each well in this data, do the following:\n",
    "\n",
    "    - For each DNA fragment in the fragments to combine, find the well and plate it is located in from the `fragments.csv`.\n",
    "\n",
    "    - Using this information, pipette **10 uL of each fragment** to the target well in the work order list. **You should use a separate tip for each fragment.**\n",
    "\n",
    "    - Finally, **mix** the **30 uL** contents in the target well together and move on to the next well\n",
    "\n",
    "Since you have now gained experience in writing liquid handling protocols, we will not provide any **strict guidelines on helper functions** or structure of the protocol for this exercise. \n",
    "\n",
    "However, do not that your code will still be graded for its **efficiency** and **quality of comments** At a minimum, you should aim to have (and we will look for):\n",
    "\n",
    "- A function to perform a mixing operation.\n",
    "\n",
    "- A function to perform one entire pipetting step given a well ID and a triplet to combine.\n",
    "\n",
    "- A function to find the appropriate well given a fragment ID.\n",
    "\n",
    "For each function you write, make sure to include **robust comments** as to how it works. It may be helpful to look back at **workshop 2** for inspiration of function structure.\n",
    "\n",
    "Your final protocol should run through the `run_protocol_exercise_3()` function.\n",
    "\n",
    "Comment your code well and include all imports **not included at the top of the notebook** that you may need. \n",
    "\n",
    "**Include a description of each helper function in the commented section below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1025e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ... from ... # add any imports needed\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "\n",
    "# Function N Code:\n",
    "# Function N Description: \n",
    "\n",
    "# Function N Code:\n",
    "# Function N Description: \n",
    "\n",
    "# Function N Code:\n",
    "# Function N Description: \n",
    "\n",
    "\n",
    "async def run_protocol_exercise_3(deck: Deck,\n",
    "                                  lh: LiquidHandler,\n",
    "                                  fragment_df_path: str | os.PathLike):\n",
    "\n",
    "    # for each well fragment work order, if the fragments returned are available, then run the code\n",
    "    for well_id, fragments in extract_fragment_combinations(work_order_csv_path, csv_path):\n",
    "        if fragments is not None:\n",
    "            ...\n",
    "        # otherwise, one of the fragments is misssing, so continue on\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# setup the deck\n",
    "deck = await make_golden_gate_ot2()\n",
    "lh = await visualize_deck(deck, LiquidHandlerChatterboxBackend())\n",
    "\n",
    "# add fragments\n",
    "add_fragments(deck, csv_path)\n",
    "\n",
    "# run protocol\n",
    "await run_protocol_exercise_3(deck, lh, csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be123c2",
   "metadata": {},
   "source": [
    "Submit a GIF of your protocol running as `exercise_3.gif` and a code file of your code **and function explanations** as `exercise_3.txt`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1a9f5b",
   "metadata": {},
   "source": [
    "### **Exercise 4.** State Verification (15 pts)\n",
    "\n",
    "Now that you have completed your experiment, you should utilize liquid tracking to write a function `verify_results()` which is able to iterate over ll the original work lists and verify, for each order, whether or not it was completed successfully.\n",
    "\n",
    "To do this, simpoly:\n",
    "\n",
    "- Iterate over the work list for each well.\n",
    "\n",
    "- Determine, for that well, if it contains all 3 of the requisite liquids by checking against the provided tuple.\n",
    "\n",
    "- If it was **not successful** (i.e. skipped because one of the clones was missing), then add the well_id to the `well_ids` list and add each missing fragment id to the growing list of `x_fragment`, `y_fragment`, and `x_fragment`.\n",
    "\n",
    "- The function provided will save your results to an output `exercise_4.csv` file. Submit this along with the code for exercise 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e7ad19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_results(cloning_csv_path : str,\n",
    "                   deck: Deck):\n",
    "\n",
    "    ... # YOUR CODE HERE\n",
    "\n",
    "    # lists to store empty wells and fragment tuples for saving as a DataFrame\n",
    "    empty_wells = []\n",
    "    fragment_x_list = []\n",
    "    fragment_y_list = []\n",
    "    fragment_z_list = []\n",
    "\n",
    "    # for each work order\n",
    "    for ... in ...:\n",
    "\n",
    "        liquids = ...\n",
    "\n",
    "        if len(liquids) == 0:\n",
    "            empty_wells.append(...)\n",
    "            fragment_x_list.append(...)\n",
    "            fragment_y_list.append(...)\n",
    "            fragment_z_list.append(...)\n",
    "        \n",
    "        # otherwise, assert that all the liquids match the appropriate names\n",
    "        else:\n",
    "            for ... in ...:\n",
    "                assert fragment == liquid[0], \"liquid did not match!\"\n",
    "    \n",
    "    # convert results to a dataframe\n",
    "    result = pd.DataFrame(\n",
    "        {\n",
    "            \"well_id\" : empty_wells,\n",
    "            \"fragment_x\" : fragment_x_list,\n",
    "            \"fragment_y\" : fragment_y_list,\n",
    "            \"fragment_z\" : fragment_z_list,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "# save data to file\n",
    "save_path = os.path.join(os.path.dirname(cwd), \"workshop_4_data\", \"exercise_4.csv\")\n",
    "result_df = verify_results(work_order_csv_path, deck)\n",
    "result_df.to_csv(save_path, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bbf99d",
   "metadata": {},
   "source": [
    "Submit the resulting data file as `exercise_4.csv` and the code as `exercise_4.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11d979c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "That's all for workshop 4! Double check that you have submitted a `.txt` file for every problem, a `.gif` file for problem 3 and a `.csv` file for exercise 4. You should have submitted:\n",
    "\n",
    "- `exercise_1.txt` with the code for exercise 1.\n",
    "\n",
    "- `exercise_2.txt` with the code for exercise 2.\n",
    "\n",
    "- `exercise_2.txt` with the code for exercise 3.\n",
    "\n",
    "- `exercise_1.gif` showing the full protocol in exercise 3, starting with the set up OT-2 deck with DNA clones.\n",
    "\n",
    "- `exercise_4.txt` with the code for exercise 4.\n",
    "\n",
    "- `exercise_4.csv` with the resulting data from running your code in exercise 4.\n",
    "\n",
    "**EVERY CODE BLOCK SHOULD HAVE WELL-WRITTEN COMMENTS**\n",
    "\n",
    "If you are still feeling unsure on deck setup, please **reach out to the teaching team**, contact info for whom can be found in the .`README.md` file on the [class GitHub](https://github.com/chory-lab/bme590-fall-2025)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab-automation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
